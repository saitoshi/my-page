{"ast":null,"code":"var _jsxFileName = \"/Users/saitoshi/Documents/GitHub/my-page/src/components/project/Internaloha.jsx\";\nimport React from \"react\";\nimport { Header, Container, Grid, Image } from 'semantic-ui-react';\nimport { NavLink } from 'react-router-dom';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\nfunction Internaloha() {\n  return /*#__PURE__*/_jsxDEV(Container, {\n    children: [/*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 8,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      textAlign: \"center\",\n      children: \"Internaloha 2.0\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 9,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Grid, {\n      textAlign: \"center\",\n      children: /*#__PURE__*/_jsxDEV(\"p\", {\n        textAlign: \"center\",\n        children: [\" \", /*#__PURE__*/_jsxDEV(\"b\", {\n          children: \" Software Engineering, TypeScript, Scrapers \"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 11,\n          columnNumber: 34\n        }, this), \" \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 11,\n          columnNumber: 86\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 11,\n        columnNumber: 11\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 10,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Grid, {\n      textAlign: \"center\",\n      children: /*#__PURE__*/_jsxDEV(\"p\", {\n        children: [/*#__PURE__*/_jsxDEV(\"span\", {\n          className: \"bold-list\",\n          children: \" 2021-12-10\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 14,\n          columnNumber: 14\n        }, this), \" \"]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 14,\n        columnNumber: 11\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 13,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Container, {\n      text: true,\n      children: [/*#__PURE__*/_jsxDEV(\"hr\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 17,\n        columnNumber: 11\n      }, this), \"For my Capstone Project course (\", /*#__PURE__*/_jsxDEV(\"a\", {\n        href: \"https://manoa.hawaii.edu/catalog/courses/ics-496-capstone-project-3/\",\n        children: \"ICS 469\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 18,\n        columnNumber: 43\n      }, this), \") I have teamed up with Professor Johnson with the development of Internaloha Scrappers.\", /*#__PURE__*/_jsxDEV(\"ul\", {\n        children: [/*#__PURE__*/_jsxDEV(\"li\", {\n          children: [\"Group Organization: \", /*#__PURE__*/_jsxDEV(\"a\", {\n            href: \"https://internaloha.github.io/documentation/\",\n            children: \"LINK\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 20,\n            columnNumber: 37\n          }, this)]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 20,\n          columnNumber: 13\n        }, this), /*#__PURE__*/_jsxDEV(\"li\", {\n          children: [\"GitHub Repository: \", /*#__PURE__*/_jsxDEV(\"a\", {\n            href: \"https://github.com/internaloha/scrapers\",\n            children: \"LINK\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 21,\n            columnNumber: 36\n          }, this)]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 21,\n          columnNumber: 13\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 19,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"b\", {\n        children: /*#__PURE__*/_jsxDEV(\"u\", {\n          children: \"Overview of Internaloha\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 23,\n          columnNumber: 14\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 23,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 23,\n        columnNumber: 48\n      }, this), \"Initially, University of Hawaii Computer Science undergraduates were able to find internships through the previous version internship page of RadGrad. However, since this version needed each internship to be manually entered. In order to spread internship awareness efficiently, Internaloha was created. Internaloha is an automated scraper system of collecting internships from a variety of job searching websites and having them displayed on the RadGrad internship explorer page. Version 1 uses Javascript and the tool Puppeteer. A node.js library which uses Chromium dev tool, Puppeteer runs headless on default. Each scraper goes to a page and then manually searches through the site for internships. Eventually data is stored inside a JSON file. Finally, the data on the JSON file is manually transferred to RadGrad\\u2019s internship explorer page.\", /*#__PURE__*/_jsxDEV(Image, {\n        src: \"https://github.com/saitoshi/images/blob/main/internaloha/radgrad.png?raw=true\",\n        size: \"large\",\n        centered: true\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 26,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"comment\",\n        children: \" A sample internship explorer on the RadGrad application.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 27,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 28,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"b\", {\n        children: /*#__PURE__*/_jsxDEV(\"u\", {\n          children: \"Purpose\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 29,\n          columnNumber: 14\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 29,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 29,\n        columnNumber: 32\n      }, this), \"The main purpose of this project was to update the previous scrappers by using TypeScript and a common format. Previously, the scrappers were written by a different group of students where the basic format was not necessarily uniform. For the Fall 2021, our main goal was to update is using classes and making it more unified within the different scrappers. \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 31,\n        columnNumber: 134\n      }, this), \"Using the \", /*#__PURE__*/_jsxDEV(\"a\", {\n        href: \"https://github.com/internaloha/scrapers/projects/1\",\n        children: \"Project Board\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 32,\n        columnNumber: 21\n      }, this), \" function from GitHub, each student worked on a certain issue regarding to the scrappers.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 33,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"b\", {\n        children: /*#__PURE__*/_jsxDEV(\"u\", {\n          children: \"Contribution to Internaloha Version 2\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 34,\n          columnNumber: 14\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 34,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 35,\n        columnNumber: 11\n      }, this), \"Below are the different scrappers and a brief summary associated to it. \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 36,\n        columnNumber: 83\n      }, this), /*#__PURE__*/_jsxDEV(\"b\", {\n        children: /*#__PURE__*/_jsxDEV(\"a\", {\n          href: \"https://github.com/internaloha/scrapers/blob/main/scrapers/Scraper.ziprecruiter.ts\",\n          children: \"ZipRecruiter Scrapper\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 37,\n          columnNumber: 14\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 37,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 37,\n        columnNumber: 136\n      }, this), \"ZipRecruiter, was the first scraper I wrote with the new format. Initially, my process was to translate the code written in version 1 of the same ZipRecruiter and then run it. However, after the first trial I noticed that that was not an option. To begin with, the HTML code that was written in version 1 did not match the current ZipRecruiter set up. Furthermore, there was a for loop within the code that did not make any sense as iterated within the same item five times.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 41,\n        columnNumber: 11\n      }, this), \"Compared to the other scrapers, writing ZipRecruiter took a longer time to write. Furthermore, the difficulty here was having the scraper keep scrolling through the page until there are no available jobs. With the new format, ZipRecruiter was written using 48 lines of code where version 1 of the same website required 130 lines of code.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 44,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(Image, {\n        className: \"ui centered big image\",\n        src: \"https://github.com/saitoshi/images/blob/main/internaloha/zipcomp.png?raw=true\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 45,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"comment\",\n        children: \"A screenshot of the code of Version 1 (on left) and Version 2 (on right) of Zip Recruiter Scraper.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 46,\n        columnNumber: 13\n      }, this), \"Overall, working on ZipRecruiter scraper provided a basic understanding on creating scrapers on the new format.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 50,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"b\", {\n        children: /*#__PURE__*/_jsxDEV(\"a\", {\n          href: \"https://github.com/internaloha/scrapers/blob/main/scrapers/Scraper.monster.ts\",\n          children: \"Monster Scrapper\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 51,\n          columnNumber: 14\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 51,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 51,\n        columnNumber: 128\n      }, this), \"For the second scraper, I wrote a scraper for Monster. The challenging part of Monster was to have it iterate it through multiple pages through a tab. Monster was similar to ZipRecruiter where the as long as there are information, the page can keep scrolling down through the list of internships. Unlike ZipRecruiter or other job websites, there was no div section name to create a while loop for. However, when looking at the source code using Google Chrome, there existed a div button where it stored the next section of listings. Thus, the scraper will keep extracting URL as long as the button exists.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 53,\n        columnNumber: 11\n      }, this), \"Furthermore, the Monster page is set up where all of the necessary information is spread out. Thus, the scraper collects the URL, position title, and company name from the main page. Then, the scraper goes to the individual URL of that specific position title and extracts the description of the job and the job location. While the job location was written in the main page, it was simpler to extract it from the individual position page for an easier to readable code.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 55,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"b\", {\n        children: /*#__PURE__*/_jsxDEV(\"a\", {\n          href: \"https://github.com/internaloha/scrapers/blob/main/scrapers/Scraper.glassdoor.ts\",\n          children: \"GlassDoor Scrapper\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 56,\n          columnNumber: 14\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 56,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 56,\n        columnNumber: 130\n      }, this), \"This was the third scrapper I have worked on. \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 57,\n        columnNumber: 57\n      }, this), \"In the original scrapper GlassDoor took one to two hours to scrape all the necessary information. After writing two scrappers writing the scrapers itself did not take a long time compared to the previous two. However, unlike the two scrapers, the scraper only took the url from the main page and rest of the necessary information (position title, company, description, etc.) was scrapped from going to that certain URL. \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 60,\n        columnNumber: 56\n      }, this), \"Furthermore, since the GlassDoor scrapper has multiple pages depending on the week, I created a for loop by scrapping the number of pages.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 62,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 63,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"b\", {\n        children: /*#__PURE__*/_jsxDEV(\"a\", {\n          href: \"https://github.com/internaloha/scrapers/blob/403de67709b04523503e6cb516c8555ef3dbdb73/scrapers/Scraper.stackoverflow.ts\",\n          children: \"StackOverFlow Scrapper\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 64,\n          columnNumber: 14\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 64,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 64,\n        columnNumber: 175\n      }, this), \"This was the fourth scrapper I have worked on. \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 65,\n        columnNumber: 58\n      }, this), \"Initially, the pattern was similar to others so writing it initially was not bad. However, after doing a test scrape I noticed that the total jobs scrapped were greater than the amount of internships available. I noticed that the scrapper scraped the recommended jobs. Therefore, to avoid this I extracted the number of internships listed on the page and then did the for loop based on that instead of the urls.length.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 68,\n        columnNumber: 11\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 16,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"push\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 71,\n      columnNumber: 9\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 7,\n    columnNumber: 7\n  }, this);\n}\n\n_c = Internaloha;\nexport default Internaloha;\n\nvar _c;\n\n$RefreshReg$(_c, \"Internaloha\");","map":{"version":3,"sources":["/Users/saitoshi/Documents/GitHub/my-page/src/components/project/Internaloha.jsx"],"names":["React","Header","Container","Grid","Image","NavLink","Internaloha"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,SAASC,MAAT,EAAiBC,SAAjB,EAA4BC,IAA5B,EAAkCC,KAAlC,QAA+C,mBAA/C;AACA,SAASC,OAAT,QAAwB,kBAAxB;;;AAEA,SAASC,WAAT,GAAuB;AACrB,sBACI,QAAC,SAAD;AAAA,4BACE;AAAA;AAAA;AAAA;AAAA,YADF,eAEE,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAE,IAAZ;AAAiB,MAAA,SAAS,EAAC,QAA3B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAFF,eAGE,QAAC,IAAD;AAAM,MAAA,SAAS,EAAG,QAAlB;AAAA,6BACE;AAAG,QAAA,SAAS,EAAC,QAAb;AAAA,qCAAuB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAAvB,oBAA2E;AAAA;AAAA;AAAA;AAAA,gBAA3E;AAAA;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,YAHF,eAME,QAAC,IAAD;AAAM,MAAA,SAAS,EAAC,QAAhB;AAAA,6BACE;AAAA,gCAAG;AAAM,UAAA,SAAS,EAAC,WAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAAH;AAAA;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,YANF,eASE,QAAC,SAAD;AAAW,MAAA,IAAI,MAAf;AAAA,8BACE;AAAA;AAAA;AAAA;AAAA,cADF,mDAEkC;AAAG,QAAA,IAAI,EAAE,sEAAT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAFlC,2GAGE;AAAA,gCACE;AAAA,0DAAwB;AAAG,YAAA,IAAI,EAAG,8CAAV;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBAAxB;AAAA;AAAA;AAAA;AAAA;AAAA,gBADF,eAEE;AAAA,yDAAuB;AAAG,YAAA,IAAI,EAAE,yCAAT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBAAvB;AAAA;AAAA;AAAA;AAAA;AAAA,gBAFF;AAAA;AAAA;AAAA;AAAA;AAAA,cAHF,eAOE;AAAA,+BAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAH;AAAA;AAAA;AAAA;AAAA,cAPF,eAOuC;AAAA;AAAA;AAAA;AAAA,cAPvC,u2BAUE,QAAC,KAAD;AAAO,QAAA,GAAG,EAAE,+EAAZ;AAA6F,QAAA,IAAI,EAAC,OAAlG;AAA0G,QAAA,QAAQ;AAAlH;AAAA;AAAA;AAAA;AAAA,cAVF,eAWE;AAAK,QAAA,SAAS,EAAC,SAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAXF,eAYE;AAAA;AAAA;AAAA;AAAA,cAZF,eAaE;AAAA,+BAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAH;AAAA;AAAA;AAAA;AAAA,cAbF,eAauB;AAAA;AAAA;AAAA;AAAA,cAbvB,yXAe6H;AAAA;AAAA;AAAA;AAAA,cAf7H,6BAgBY;AAAG,QAAA,IAAI,EAAE,oDAAT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAhBZ,4GAiBE;AAAA;AAAA;AAAA;AAAA,cAjBF,eAkBE;AAAA,+BAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAH;AAAA;AAAA;AAAA;AAAA,cAlBF,eAmBE;AAAA;AAAA;AAAA;AAAA,cAnBF,2FAoB0E;AAAA;AAAA;AAAA;AAAA,cApB1E,eAqBE;AAAA,+BAAG;AAAG,UAAA,IAAI,EAAC,oFAAR;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAH;AAAA;AAAA;AAAA;AAAA,cArBF,eAqB+H;AAAA;AAAA;AAAA;AAAA,cArB/H,6eAyBE;AAAA;AAAA;AAAA;AAAA,cAzBF,oWA4BE;AAAA;AAAA;AAAA;AAAA,cA5BF,eA6BE,QAAC,KAAD;AAAO,QAAA,SAAS,EAAC,uBAAjB;AAAyC,QAAA,GAAG,EAAC;AAA7C;AAAA;AAAA;AAAA;AAAA,cA7BF,eA8BI;AAAK,QAAA,SAAS,EAAC,SAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cA9BJ,kIAkCI;AAAA;AAAA;AAAA;AAAA,cAlCJ,eAmCE;AAAA,+BAAG;AAAG,UAAA,IAAI,EAAG,+EAAV;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAH;AAAA;AAAA;AAAA;AAAA,cAnCF,eAmCuH;AAAA;AAAA;AAAA;AAAA,cAnCvH,gnBAqCE;AAAA;AAAA;AAAA;AAAA,cArCF,weAuCE;AAAA;AAAA;AAAA;AAAA,cAvCF,eAwCE;AAAA,+BAAG;AAAG,UAAA,IAAI,EAAC,iFAAR;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAH;AAAA;AAAA;AAAA;AAAA,cAxCF,eAwCyH;AAAA;AAAA;AAAA;AAAA,cAxCzH,iEAyCgD;AAAA;AAAA;AAAA;AAAA,cAzChD,ubA4C+C;AAAA;AAAA;AAAA;AAAA,cA5C/C,6JA8CE;AAAA;AAAA;AAAA;AAAA,cA9CF,eA+CE;AAAA;AAAA;AAAA;AAAA,cA/CF,eAgDE;AAAA,+BAAG;AAAG,UAAA,IAAI,EAAE,yHAAT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAH;AAAA;AAAA;AAAA;AAAA,cAhDF,eAgDsK;AAAA;AAAA;AAAA;AAAA,cAhDtK,kEAiDiD;AAAA;AAAA;AAAA;AAAA,cAjDjD,qbAoDE;AAAA;AAAA;AAAA;AAAA,cApDF;AAAA;AAAA;AAAA;AAAA;AAAA,YATF,eAgEE;AAAK,MAAA,SAAS,EAAE;AAAhB;AAAA;AAAA;AAAA;AAAA,YAhEF;AAAA;AAAA;AAAA;AAAA;AAAA,UADJ;AAsED;;KAvEQA,W;AAyET,eAAeA,WAAf","sourcesContent":["import React from \"react\";\nimport { Header, Container, Grid, Image } from 'semantic-ui-react';\nimport { NavLink } from 'react-router-dom';\n\nfunction Internaloha() {\n  return (\n      <Container>\n        <br/>\n        <Header as =\"h2\" textAlign=\"center\">Internaloha 2.0</Header>\n        <Grid textAlign = \"center\">\n          <p textAlign=\"center\"> <b> Software Engineering, TypeScript, Scrapers </b> <br/></p>\n        </Grid>\n        <Grid textAlign=\"center\">\n          <p><span className=\"bold-list\"> 2021-12-10</span> </p>\n        </Grid>\n        <Container text>\n          <hr/>\n          For my Capstone Project course (<a href =\"https://manoa.hawaii.edu/catalog/courses/ics-496-capstone-project-3/\">ICS 469</a>) I have teamed up with Professor Johnson with the development of Internaloha Scrappers.\n          <ul>\n            <li>Group Organization: <a href = \"https://internaloha.github.io/documentation/\">LINK</a></li>\n            <li>GitHub Repository: <a href =\"https://github.com/internaloha/scrapers\">LINK</a></li>\n          </ul>\n          <b><u>Overview of Internaloha</u></b><br/>\n          Initially, University of Hawaii Computer Science undergraduates were able to find internships through the previous version internship page of RadGrad. However, since this version needed each internship to be manually entered. In order to spread internship awareness efficiently, Internaloha was created.\n          Internaloha is an automated scraper system of collecting internships from a variety of job searching websites and having them displayed on the RadGrad internship explorer page. Version 1 uses Javascript and the tool Puppeteer. A node.js library which uses Chromium dev tool, Puppeteer runs headless on default. Each scraper goes to a page and then manually searches through the site for internships. Eventually data is stored inside a JSON file. Finally, the data on the JSON file is manually transferred to RadGrad’s internship explorer page.\n          <Image src={\"https://github.com/saitoshi/images/blob/main/internaloha/radgrad.png?raw=true\"} size='large' centered/>\n          <div className=\"comment\"> A sample internship explorer on the RadGrad application.</div>\n          <br/>\n          <b><u>Purpose</u></b><br/>\n          The main purpose of this project was to update the previous scrappers by using TypeScript and a common format. Previously, the scrappers were written by a different group of students where the basic format was not necessarily uniform.\n          For the Fall 2021, our main goal was to update is using classes and making it more unified within the different scrappers. <br/>\n          Using the <a href =\"https://github.com/internaloha/scrapers/projects/1\">Project Board</a> function from GitHub, each student worked on a certain issue regarding to the scrappers.\n          <br/>\n          <b><u>Contribution to Internaloha Version 2</u></b>\n          <br />\n          Below are the different scrappers and a brief summary associated to it. <br />\n          <b><a href=\"https://github.com/internaloha/scrapers/blob/main/scrapers/Scraper.ziprecruiter.ts\">ZipRecruiter Scrapper</a></b><br />\n          ZipRecruiter, was the first scraper I wrote with the new format. Initially, my process was to translate the code written in version 1 of the same ZipRecruiter and then run it. However, after the first trial I noticed that that was\n          not an option.\n          To begin with, the HTML code that was written in version 1 did not match the current ZipRecruiter set up. Furthermore, there was a for loop within the code that did not make any sense as iterated within the same item five times.\n          <br />\n          Compared to the other scrapers, writing ZipRecruiter took a longer time to write. Furthermore, the difficulty here was having the scraper keep scrolling through the page until there are no available jobs. With the new format,\n          ZipRecruiter was written using 48 lines of code where version 1 of the same website required 130 lines of code.\n          <br />\n          <Image className=\"ui centered big image\" src=\"https://github.com/saitoshi/images/blob/main/internaloha/zipcomp.png?raw=true\"/>\n            <div className=\"comment\">\n              A screenshot of the code of Version 1 (on left) and Version 2 (on right) of Zip Recruiter Scraper.\n            </div>\n            Overall, working on ZipRecruiter scraper provided a basic understanding on creating scrapers on the new format.\n            <br/>\n          <b><a href = \"https://github.com/internaloha/scrapers/blob/main/scrapers/Scraper.monster.ts\">Monster Scrapper</a></b><br/>\n          For the second scraper, I wrote a scraper for Monster. The challenging part of Monster was to have it iterate it through multiple pages through a tab. Monster was similar to ZipRecruiter where the as long as there are information, the page can keep scrolling down through the list of internships. Unlike ZipRecruiter or other job websites, there was no div section name to create a while loop for. However, when looking at the source code using Google Chrome, there existed a div button where it stored the next section of listings. Thus, the scraper will keep extracting URL as long as the button exists.\n          <br/>\n          Furthermore, the Monster page is set up where all of the necessary information is spread out. Thus, the scraper collects the URL, position title, and company name from the main page. Then, the scraper goes to the individual URL of that specific position title and extracts the description of the job and the job location. While the job location was written in the main page, it was simpler to extract it from the individual position page for an easier to readable code.\n          <br/>\n          <b><a href=\"https://github.com/internaloha/scrapers/blob/main/scrapers/Scraper.glassdoor.ts\">GlassDoor Scrapper</a></b><br/>\n          This was the third scrapper I have worked on. <br/>\n          In the original scrapper GlassDoor took one to two hours to scrape all the necessary information. After writing two scrappers writing the scrapers itself did not\n          take a long time compared to the previous two. However, unlike the two scrapers, the scraper only took the url from the main page and rest of the necessary information (position title, company, description, etc.)\n          was scrapped from going to that certain URL. <br/>\n          Furthermore, since the GlassDoor scrapper has multiple pages depending on the week, I created a for loop by scrapping the number of pages.\n          <br/>\n          <br/>\n          <b><a href =\"https://github.com/internaloha/scrapers/blob/403de67709b04523503e6cb516c8555ef3dbdb73/scrapers/Scraper.stackoverflow.ts\">StackOverFlow Scrapper</a></b><br/>\n          This was the fourth scrapper I have worked on. <br/>\n          Initially, the pattern was similar to others so writing it initially was not bad. However, after doing a test scrape I noticed that the total jobs scrapped were greater than the amount of internships available.\n          I noticed that the scrapper scraped the recommended jobs. Therefore, to avoid this I extracted the number of internships listed on the page and then did the for loop based on that instead of the urls.length.\n          <br/>\n\n        </Container>\n        <div className =\"push\"/>\n      </Container>\n\n\n  );\n}\n\nexport default Internaloha;\n"]},"metadata":{},"sourceType":"module"}